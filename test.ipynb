{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database_utils import DatabaseConnector as dc\n",
    "from data_extraction import DataExtractor as dex\n",
    "from data_cleaning import DataCleaning as dcl\n",
    "\n",
    "import pandas as pd\n",
    "import nbformat # save as .ipynb\n",
    "\n",
    "import yaml # to read .yaml. Help with read_db_creds\n",
    "from sqlalchemy import create_engine, inspect # this ORM will transform the python objects into SQL tables\n",
    "from decouple import config #  calling sensitive information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='db_creds.yaml' # Step 1: Specify the correct file path\n",
    "database_connector = dc()\n",
    "credentials = database_connector.read_db_creds(file_path)    # Step 2: Read credentials\n",
    "engine = database_connector.init_db_engine(credentials)      # Step 3: Initialise database engine\n",
    "data_extractor = dex()\n",
    "\n",
    "# Rest of your code for extracting data and saving the notebook\n",
    "tables = data_extractor.list_db_tables(engine)   \n",
    "# Available Tables: ['legacy_store_details', 'legacy_users', 'orders_table']            \n",
    "print(\"Available Tables: \", tables)                          # Step 4: List all tables in the database\n",
    "for table_name in tables:\n",
    "\n",
    "    # Import data from the table into DataFrame\n",
    "    table_df = pd.read_sql(table_name, engine)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    csv_filename = f\"{table_name}_data.csv\"\n",
    "    table_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Saved {table_name} DataFrame as {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define a regular expression pattern\n",
    "integer_pattern = re.compile(r'\\d+')\n",
    "\n",
    "# Example values as a list\n",
    "values = ['80r', 'rt78', '101']\n",
    "\n",
    "# Iterate over each element in the list\n",
    "for value in values:\n",
    "    # Use search method to find the first match of the pattern in the string\n",
    "    match = integer_pattern.search(str(value))\n",
    "\n",
    "    # Check if a match is found\n",
    "    if match:\n",
    "        # Access the matched portion using group()\n",
    "        matched_value = match.group()\n",
    "        print(f\"Original value: {value}\")\n",
    "        print(f\"Matched value: {matched_value}\")\n",
    "        print(matched_value)\n",
    "    else:\n",
    "        print(f\"No match found for {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula # read tables in a PDF\n",
    "\n",
    "\n",
    "def retrieve_pdf_data(pdf_path = \"https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf\"):\n",
    "    card_data_df = tabula.read_pdf(pdf_path, stream=True)\n",
    "    return card_data_df\n",
    "\n",
    "card_data_df = retrieve_pdf_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "database_connector = dc()\n",
    "credentials = database_connector.read_db_creds()  \n",
    "\n",
    "api_key = f\"{credentials['api_key']}\"\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX\n"
     ]
    }
   ],
   "source": [
    "api_key = config('api_key')\n",
    "cred_dict = dc.read_db_creds(file_path = api_key)\n",
    "key = cred_dict['api_key']\n",
    "\n",
    "headers = {'x-api-key': key}\n",
    "\n",
    "print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "password = config('mypassword')\n",
    "print(password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "password = config('api_key')\n",
    "print(password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #API section\n",
    "\n",
    "    api_key = config('api_key')\n",
    "    api_connector = dc()\n",
    "    cred_api = api_connector.read_db_creds(file_path = api_key)\n",
    "    x_api_key = cred_api['api_key']\n",
    "\n",
    "    headers = {'x-api-key': x_api_key}\n",
    "\n",
    "    number_of_stores_endpoint = 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores'\n",
    "    retrieve_a_store_endpoint = 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/{store_number}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 'products_details' as 'products_details.csv'.\n",
      "An error occurred: read_csv() got an unexpected keyword argument 'index'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m s3_address \u001b[38;5;241m=\u001b[39m cred_api[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3_address\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# access the .yaml key\u001b[39;00m\n\u001b[0;32m     27\u001b[0m local_file_path \u001b[38;5;241m=\u001b[39m cred_api[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_file_path\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# specifies the desired local path to save the file\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m products_df, table_name, csv_filename \u001b[38;5;241m=\u001b[39m api_extractor\u001b[38;5;241m.\u001b[39mextract_from_s3(s3_address, local_file_path)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, shall be extracted. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(products_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "from database_utils import DatabaseConnector as dc\n",
    "from data_extraction import DataExtractor as dex\n",
    "from data_cleaning import DataCleaning as dcl\n",
    "# import missingno as msno # Visualising Missing Data\n",
    "# import plotly.express as px # Visualising histogram\n",
    "\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "import nbformat # save as .ipynb\n",
    "\n",
    "from decouple import config #  calling sensitive information\n",
    "import yaml # to read .yaml. Help with read_db\n",
    "\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# Retrieve PDF from AWS S3 bucket and convert to CSV\n",
    "api_extractor = dex() \n",
    "\n",
    "# to gain access to private credentials for API\n",
    "cred_access = config('credentials_env') # refers to .yaml file ## from decouple import config\n",
    "cred_api = api_connector.read_db_creds(file_path = cred_access) # extracts the .yaml file\n",
    "\n",
    "s3_address = cred_api['s3_address'] # access the .yaml key\n",
    "local_file_path = cred_api['local_file_path'] # specifies the desired local path to save the file\n",
    "products_df, table_name, csv_filename = api_extractor.extract_from_s3(s3_address, local_file_path)\n",
    "\n",
    "print(f\"'{table_name}', shall be extracted. \\n\")\n",
    "print(products_df, \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without limit: ['s3:', '', 'data-handling-public', 'products', 'details', 'file.csv']\n",
      "With limit: ['s3:', '/data-handling-public/products/details/file.csv']\n"
     ]
    }
   ],
   "source": [
    "s3_address = 's3://data-handling-public/products/details/file.csv'\n",
    "\n",
    "# Without specifying the maximum number of splits\n",
    "without_limit = s3_address.split('/')\n",
    "print(\"Without limit:\", without_limit)\n",
    "\n",
    "# Specifying the maximum number of splits as 1\n",
    "with_limit = s3_address.split('/', 1)\n",
    "print(\"With limit:\", with_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3212241868.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Without limit: ['s3:', '', 'data-handling-public', 'products', 'details', 'file.csv']\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Without limit: ['s3:', '', 'data-handling-public', 'products', 'details', 'file.csv']\n",
    "With limit: ['s3:', 'data-handling-public/products/details/file.csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without limit: ['s3:', '', 'data-handling-public', 'products', 'details', 'file.csv']\n",
      "With limit: ['s3:', '', 'data-handling-public', 'products/details/file.csv']\n"
     ]
    }
   ],
   "source": [
    "s3_address = 's3://data-handling-public/products/details/file.csv'\n",
    "without_limit = s3_address.split('/')\n",
    "with_limit = s3_address.split('/', 3)\n",
    "print(\"Without limit:\", without_limit)\n",
    "print(\"With limit:\", with_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without limit: ['https:', '', 'data-handling-public.s3.eu-west-1.amazonaws.com', 'date_details.json']\n",
      "With limit: ['https:', '', 'data-handling-public.s3.eu-west-1.amazonaws.com', 'date_details.json']\n"
     ]
    }
   ],
   "source": [
    "s3_address = 'https://data-handling-public.s3.eu-west-1.amazonaws.com/date_details.json'\n",
    "without_limit = s3_address.replace('s3://', 'https://').split('/')\n",
    "with_limit = s3_address.split('/', 3)\n",
    "\n",
    "print(\"Without limit:\", without_limit)\n",
    "print(\"With limit:\", with_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_address_data_events: 'https://data-handling-public.s3.eu-west-1.amazonaws.com/date_details.json'\n",
    "without_limit = s3_address.split('/')\n",
    "bucket_name, object_key = s3_address_data_events.replace(['s3://', ''], ['https://', '']).split('/', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_from_s3(s3_address, local_file_path):\n",
    "    try:\n",
    "        # Create S3 client\n",
    "        s3 = boto3.client('s3')\n",
    "\n",
    "        # Extract bucket name and object key from S3 address\n",
    "        if 's3://' in s3_address:\n",
    "            bucket_name, object_key = s3_address.replace('s3://', '').split('/', 1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid S3 address format\")\n",
    "\n",
    "        # Download the file from S3 to the local machine\n",
    "        s3.download_file(bucket_name, object_key, local_file_path)\n",
    "\n",
    "        # Determine the table_name based on the file type\n",
    "        if '.json' in object_key:\n",
    "            table_name = object_key.replace('.json', '')\n",
    "        else:\n",
    "            table_name = object_key  # Use the original object_key if no replacement is needed\n",
    "\n",
    "        # Read the JSON file into a Pandas DataFrame\n",
    "        if '.json' in object_key:\n",
    "            with open(local_file_path, 'r') as json_file:\n",
    "                data_df = pd.json_normalize(json.load(json_file))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(f\"\\nExtracted data from '{table_name}':\\n\")\n",
    "        print(data_df)\n",
    "\n",
    "        # Save the DataFrame as a CSV file\n",
    "        csv_filename = f\"{table_name}.csv\"\n",
    "        data_df.to_csv(csv_filename)\n",
    "        print(f\"\\nSaved '{table_name}' as '{csv_filename}'.\")\n",
    "\n",
    "        # Create a new notebook\n",
    "        notebook = nbformat.v4.new_notebook()\n",
    "        # Add a code cell for the table to the notebook\n",
    "        code_cell = nbformat.v4.new_code_cell(f\"import pandas as pd\\n\"\n",
    "                                            f\"# Import data from '{csv_filename}' into DataFrame.\\n\"\n",
    "                                            f\"table_name = '{table_name}'\\n\"\n",
    "                                            f\"csv_file_path = '{table_name}.csv'\\n\"\n",
    "                                            f\"{table_name}_df = pd.read_csv(csv_file_path)\\n\"\n",
    "                                            f\"# Display the DataFrame\\n\"\n",
    "                                            f\"display({table_name}_df)\")\n",
    "\n",
    "        notebook.cells.append(code_cell)\n",
    "\n",
    "        # Save the notebook to a .ipynb file with a name based on the fixed extracted table\n",
    "        notebook_file = f\"{table_name}.ipynb\"\n",
    "        with open(notebook_file, 'w') as nb_file:\n",
    "            nbformat.write(notebook, nb_file)\n",
    "            print(f\"\\nSaved '{table_name}' as '{notebook_file}'.\\n\")\n",
    "\n",
    "        return data_df, table_name, local_file_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred: Invalid S3 address format\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m s3_address \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://data-handling-public.s3.eu-west-1.amazonaws.com/date_details.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m local_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/local/file/date_details.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m data_df, table_name, csv_filename \u001b[38;5;241m=\u001b[39m extract_from_s3(s3_address, local_file_path)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Usage\n",
    "s3_address = 'https://data-handling-public.s3.eu-west-1.amazonaws.com/date_details.json'\n",
    "local_file_path = 'path/to/local/file/date_details.json'\n",
    "\n",
    "data_df, table_name, csv_filename = extract_from_s3(s3_address, local_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrdc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
